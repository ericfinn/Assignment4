#! usr/bin/env/ python
from __future__ import division

import math
import decisionTreeNode
import data

classification = []

#attribute: list(string, string, set)
#examples: list(tuples)


def run(dataFile, namesFile):
	examples, attributes = data.parseFile(dataFile, namesFile)
	global classification
	classification = attributes[-1]
	#split data into the teaching set and the test set
	splitIdx = int(math.floor(len(examples) * 0.8))
	teaching = examples[:splitIdx]
	testSet = examples[splitIdx:]
	#build tree
	tree = dtl(teaching, attributes, None)
	print tree
	#use test set to test the tree
	

#function DTL(examples, attributes, default) returns a decision tree
#	if examples is empty then return default
#	else if all examples have some classification then return that classification
#	else if attributes is empty then return MODE(examples)
#	else
#		best = CHOOSE-ATTRIBUTE(attributes, examples)
#		tree = a new decision tree w/ root test = best
#		for each value v[i] of best do:
#			examples[i] = {elements of examples with best = v[i] }
#			subtree = DTL(examples[i], attributes - best, MODE(examples))
#			add a branch to tree with label v[i] and subtree subtree
#		return tree

def dtl(examples, attributes, default):
	if examples == []:
		return default
	elif len(matchingExamples(examples, classification, classOf(examples[0]))) == len(examples):
		return decisionTreeNode.Node(None, None, examples[0][-1], None)
	elif attributes == []:
		return mode(examples)
	else:
		best = chooseAttribute(attributes, examples)
		tree = decisionTreeNode.Node(None, {}, None, best)
		if best != None:
			for v in data.getDomain(best):
				subexamples = matchingExamples(examples, best, v)
				reducedAttrs = reduceAttrList(attributes, best)
				subtree = dtl(subexamples, reducedAttrs, mode(examples))
				tree.children[v] = subtree
				subtree.parent = tree
			return tree
		else:
			print "Best is None!"
			return mode(examples)

def reduceAttrList(attributes, toRemove):
	return filter(lambda attr: data.getName(attr) != data.getName(toRemove), attributes)

#returns subset of given examples in which
#the given attribute of each item in the set has a value
#which matches the given value
def matchingExamples(examples, attribute, value):
	attrIdx = data.indexOfAttribute(attribute)
	return filter(lambda ex: ex[attrIdx] == value, examples)

#given a datum (as in an element of examples), returns the classification of the datum
def classOf(datum):
	return datum[-1]

def attributeValOf(datum, attribute):
	return datum[data.indexOfAttribute(attribute)]

#function MODE(examples) returns a decision tree
#	return a new decision tree w/ answer = mode of results of examples

def mode(examples):
	count = 0
	mode = 0
	classes = map(classOf, examples)
	for ex in set(classes):
		temp_count = classes.count(ex)
		if temp_count > count:
			count = temp_count
			mode = ex
	decision = decisionTreeNode.Node(None, None, mode, None)
	return decision
#function GAIN(attribute, examples) returns number
#	gain = ENTROPY(examples)
#	for each value of attribute v:
#		weight = (number of examples with attribute = v)/(total number of examples)
#		gain = gain - weight * ENTROPY(examples with attribute = v)
#	return gain

def calcGain(attribute, examples):
	print examples
	gain = entropy(examples)
	attrVals = map(lambda x: attributeValOf(x, attribute), examples)
	for v in data.getDomain(attribute):
		print "calcGain {}".format(examples)
		weight = attrVals.count(v)/len(examples)
		temp_examples = filter(lambda i: attributeValOf(i, attribute) == v, examples)
		gain = gain - weight * entropy(temp_examples)
	return gain
#function ENTROPY(examples) returns number
#	entropy = 0
#	for each classification value v:
#		prob = (examples classified as v)/(count of all examples)
#		entropy = entropy - (prob * log_2(prob))
#	return entropy

def entropy(examples):
	entropy = 0
	prob = 0
	classes = map(classOf, examples)
	for v in classification:
		print "in entropy: {}".format(examples)
		prob = classes.count(v)/len(examples)
		#do not take log of 0 - throw out this term
		if prob == 0:
			continue
		else:
			entropy = entropy - (prob * math.log(prob,2))
	return entropy

#function CHOOSE-ATTRIBUTE(attributes, examples) returns attribute
#	return attribute with highest gain
def chooseAttribute(attributes, examples):
	gain = -1
	attr = None
	print "chooseAttribute: attributes = {}".format(attributes)
	for set in attributes:
		#do not consider ignore or answer attributes
		if data.getDomainType(set) == 'i' or data.getDomainType(set) == 'a':
			continue
		else:
			print "In chooseattr {}".format(examples)
			newGain = calcGain(set, examples)
			if newGain > gain:
				gain = newGain
				attr = set
	return attr

def accuracy(total, mistakes):
	return (total - mistakes)/total * 100

run("restaurant.data", "restaurant.names")
